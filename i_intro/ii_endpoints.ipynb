{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3d7228",
   "metadata": {},
   "source": [
    "# 1. Callling OpenAI API Again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f85e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-. Please recheck your API key.\")\n",
    "else:\n",
    "    print(\"API key found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59aed06",
   "metadata": {},
   "source": [
    "## AN ENDPOINT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b211718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-5-nano',\n",
       " 'messages': [{'role': 'user', 'content': 'Tell me a weird fact'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a weird fact\"}]\n",
    "}\n",
    "\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdda273c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-CwlKlANKGI6xW7PUERX1Yrzib6znz',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1768121011,\n",
       " 'model': 'gpt-5-nano-2025-08-07',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Bananas are technically berries, but strawberries aren’t. In botanical terms, a berry is a fleshy fruit from a single ovary, with seeds inside. Bananas fit that definition, while strawberries don’t—the red flesh is an enlarged stem (receptacle), and the “seeds” on the outside are actually tiny fruits (achenes) from multiple ovaries.',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 11,\n",
       "  'completion_tokens': 1173,\n",
       "  'total_tokens': 1184,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 1088,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b3a91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c57671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bananas are technically berries, but strawberries aren’t. In botanical terms, a berry is a fleshy fruit from a single ovary, with seeds inside. Bananas fit that definition, while strawberries don’t—the red flesh is an enlarged stem (receptacle), and the “seeds” on the outside are actually tiny fruits (achenes) from multiple ovaries.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ab1f05",
   "metadata": {},
   "source": [
    "## 2. Local LLM: Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07787dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3bedf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s one that might send chills down your spine:\\n\\nDid you know that there is a phenomenon known as \"The Dyatlov Pass Incident\"? In 1959, nine experienced hikers and mountaineers embarked on an expedition to the Ural Mountains in Russia. They were tasked with reaching the summit of Mount Otorten.\\n\\nHowever, six weeks into their journey, something went horribly wrong. The group\\'s bodies were found scattered across the mountain, with strange and unexplained injuries. Some had severe internal trauma, while others had suffered severe frostbite. However, when they found one of the victims\\' bodies, it was missing its tongue and had a severed ear.\\n\\nMoreover, the hikers left behind their campsite and trail mix, which included a bottle of wine and some food, with no signs of tampering or disturbance. When investigators searched for any sign of what might have caused this tragic event, they found that there was no animal tracks in the area, and even the group\\'s own dogs were left behind.\\n\\nThe investigation ended in a conclusion that was both intriguing and unsettling: it implied that something inexplicable occurred, which was simply beyond human understanding. The cause remains unknown to this day.\\n\\nSources suggest alternative explanations for the incident such as avalanches, avalanching caused by human error or possibly being lost due to blinding blizzard conditions on higher levels of mountains...'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\": \"user\", \"content\": \"Tell me a creepy fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059c5131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s a fun fact for you: \\n\\n\"Information is power, but it\\'s not always free.\" — Albert Einstein'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2d33a",
   "metadata": {},
   "source": [
    "Even though, we use OpenAI object to call Ollama models, the endpoint is different.\n",
    "There is no OpenAI servers involved. The requests are sent to your local machine where Ollama is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9606be4",
   "metadata": {},
   "source": [
    "# 3. EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
